---
title: "To impute, or not to impute, that is the question"
subtitle: "SDS final project"
author: "Christian Buda"
date: '2023-07-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load packages, message=FALSE, warning=FALSE, echo = FALSE}
require(mvtnorm)
require(MCMCpack)
library(progress)
library(future)
library(future.apply)
require(LaplacesDemon)
require(asbio)
require(dplyr)
require(krige)
library(ggplot2)
library(ggcorrplot)
library(R2jags)
library(ggmcmc)
library(mcmcse)
require(latex2exp)
```

```{r get colors, echo=FALSE}

##### here we make some colors for later #####


somecolornames = c('chocolate1', 'aquamarine1', 'chartreuse1', 'cornflowerblue', 'coral1', 'cyan1', 'brown1', 'darkgoldenrod1', 'darkolivegreen1', 'dodgerblue3', 'goldenrod1', 'indianred1', 'khaki', 'magenta1', 'olivedrab1', 'orangered', 'royalblue1', 'seagreen1', 'salmon', 'tomato', 'springgreen', 'steelblue', 'tan2', 'slateblue1', 'plum1')


transparent_color <- function(color, alpha = 0.4) {

  rgb_col <- col2rgb(color)/255

  col <- rgb(rgb_col[1], rgb_col[2], rgb_col[3], alpha = alpha)

  return(col)
}

somecolors = list()

for( i in 1:length(somecolornames)) {
  somecolors = append(somecolors, list(transparent_color(somecolornames[i], alpha = 0.4)))
}


```


# The What - Today's goal
For this project I decided to implement a few Bayesian data imputation strategies, starting from a standard Multivariate Normal strategy and progressively incorporating data and domain knowledge to the technique. Performances are evaluated by comparing the techniques among themselves and with the actual "missing" data.

# The Why - Data exploration
The data used comes from [CalCOFI](https://calcofi.org/about/#program-overview/), an organization that conducts quarterly cruises from north of San Francisco Bay to San Diego and extends from the coast to 500 km offshore, spanning national and international waters, to take samples of ocean water at various depths.

The data used throughout this project is the [bottle](https://calcofi.org/data/oceanographic-data/bottle-database/) data, which contains data about ocean water at various depths, but ignores the specific point at which the sample was taken.

This dataset contains several features, spanning from the most basic "Temperature" and "Salinity" of the water to quality and precision indices for each measure, up to the most extravagant "Phaeop" (the concentration of phaeopigments, non-photosynthetic pigment which is the degradation product of algal chlorophyll pigments). I only selected $5$ of these features for the analysis:

* **Depthm**: the depth (in meters) of the water sample considered. This is not technically "random", since it's part of the design of the experiment.
* **T_degC**: Temperature of the water sample (in Celsius). It is more or less constant as we go deeper in the ocean.
* **O2ml_L**: Concentration of oxygen in the water (measured in $ml$ per liter). Size and variety of marine flora and fauna heavily depends on this quantity.
* **STheta**: The potential density (measured in $\frac{kg}{m^3}$), i.e. the density of a water sample if it was adiabatically raised to the surface. The potential density quantifies how much the density of the water is due to the effect of temperature by removing the effect of pressure on the system.
* **O2Sat**: The ratio of the O2 concentration to the maximum possible concentration at a given Temperature and Salinity. This measure can also reach values higher than $100\%$ for example in situations in which microorganisms are dumping oxygen into the environment.

Loading the data makes immediately clear why this dataset is suited for this project: it is really big with its $892932$ data points, and there is lots of missing data!

```{r, echo = FALSE}
# all data
data = read.csv('194903-202010_Bottle.csv', sep = ",")

# only some columns
interesting_cols = c(5,6,8,9,10)
data = data[interesting_cols]

summary(data)

```

Since it is really big, and since the final goal is to evaluate data imputation techniques, let's take just a subset of $10000$ rows, without including any missing value.

```{r subsample dataset, echo = FALSE}
# remove NA
data = data[!apply(is.na(data), 1, any),]

set.seed(42)

# subsample dataset
n = 10000
data_idx = sample(dim(data)[1], size = n)
data = data[data_idx,]

# reorder data according to depth
data = data[order(data$Depthm),]

```

We can now explore our final dataset; let's start by looking at how the features interact among themselves:

```{r pairplot, fig.width = 11, fig.height = 10, echo = FALSE}

panel.hist <- function(x, ...) {
  usr <- par("usr")
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  col = sample(25,1)
  rect(breaks[-nB], 0, breaks[-1], y, col = somecolors[[col]], border = somecolors[[col]])
}

panel.cor <- function(x, y) {
  usr <- par("usr")
  col = floor(10*sum(x+y))%%25 + 1
  points(x, y, col = somecolors[[col]], pch = 20)
}

pairs(data, panel = panel.cor, diag.panel = panel.hist, main = "Full dataset")

```

As we could have expected, **O2Sat** and **O2ml_L** are strongly linearly correlated, and we can also identify a negative correlation between **T_degC** and **STheta**.
While **STheta** and **T_degC** "could" be modelled as normal distributions, the distributions of **O2ml_L** and **O2Sat** are highly non-Gaussian (and very similar between each other).

We can also see strong non-linear correlations between **Depthm** and the other features, this suggests a particular relevance of this variable. As already said, this measurements are not "random" but rather are planned in the design of the experiment. We can see that its distribution looks like an exponential, let's take a closer look at the distribution along with a simple estimate of its underlying distribution:

```{r, echo = FALSE}

hist(data[,1], probability = T, breaks = 50, col = somecolors[[6]], border = somecolornames[6], xlab = 'Depth', main = '')
curve(dexp(x, 1/mean(data[,1])), from = 0, to = 5000, col = somecolornames[1], lwd = 2, add = T)

```

Since this variable is not really random, it may be interesting to look at the conditional distributions of the other features given our data. Here are shown the distributions at 4 different depths, 0,10,100 and 1000 m:

```{r, fig.width = 11, fig.height = 10, echo = FALSE}

d = c(0,10,100,1000)

par(mfrow = c(4,4))

for( i in 1:4) {
  for (j in 1:4) {
    hist(data[data[,1] == d[i],j+1], main = paste('Distribution given depth =', d[i],'m'), xlab = colnames(data)[j+1], probability = T, col = somecolors[[j]], border = somecolornames[j])
  }
}


par(mfrow = c(1,1))

```


As we can see, the Gaussianity assumption on the other features is much more reasonable when conditioning on **Depthm**, this is going to be useful later on.

# The How - Models and techniques

The portion of absent values for some of these features is really high, my goal will be to take a subset of the data without missing values, masking some values and trying to impute them in different ways. To do this, let's extract a random sample from the full dataset to take a sample out of the distribution of the NA values, this is useful to create the mask.

```{r, echo = FALSE}

# all data
mask = read.csv('194903-202010_Bottle.csv', sep = ",")

# only some columns
interesting_cols = c(5,6,8,9,10)
mask = mask[interesting_cols]


set.seed(420)
mask = mask[sample(dim(data)[1], size = n),]
mask = is.na(mask)


# distribution of NA values for each row
cat('Distribution of the number of missing values in each row')
print(table(apply(mask, 1, sum)))

# set a maximum of 3 NA per row
mask[apply(mask, 1, sum)==4,] = rep(F, 5)

cat('\nNumber of missing values in each row\n')
print(colSums(mask))
```

In creating this mask, we removed the rows with 4 missing values as they are just 5 and it is unrealistic to impute so many values. As we can see, there are lots of missing values in the O2 columns, and most rows only have 2 missing values.

To evaluate the goodness of our imputation we will both visually compare the distribution and the correlations between the features with the ones obtained in the full dataset (shown above), and we will compute the MSE of the imputed data wrt the real one. To make result interpretation more convenient, the data is normalized here to have zero mean and unit variance.

```{r, echo = FALSE}
# rescale data
scaled_scale = scale(data)
data = scaled_scale
scaled_center = attr(scaled_scale, 'scaled:center')
scaled_scale = attr(scaled_scale, 'scaled:scale')

# create masked_data
masked_data = data
masked_data[mask] = NA

summary(masked_data)
```

## Model 1: the story of a naive frequentist

I decided with the simplest possible approach to data imputation: substituting each value with the mean of the corresponding column. This "frequentist" approach is really simple, we'll use it as a baseline to compare the other methods.

```{r, fig.width = 11, fig.height = 10, echo = FALSE}

X.full.frequentist = masked_data

replacements = colMeans(X.full.frequentist, na.rm = T)

for ( i in 1:dim(X.full.frequentist)[2]) {
  X.full.frequentist[is.na(X.full.frequentist[,i]), i] = replacements[i]
}

pairs(X.full.frequentist, panel = panel.cor, diag.panel = panel.hist, main = "Model 1")

```

As we can see, when the number of NA values is small, the distribution is almost unchanged, but the effect of this imputation is clearly visible in the scatter plots. When the number of NA values is consistent, the distribution is also heavily affected by the averages we added.

Let's see how it performs with respect to the "true" masked values.

```{r, fig.width = 11, fig.height = 10, echo = FALSE}

compute_MSE = function(x) {
  cat('MSE of the imputed data wrt the correct ones:\n')
  res = sqrt(colSums((x[,-1] - data[,-1])**2)/colSums(is.na(masked_data[,-1])))
  
  res2 = c(sqrt(sum((x - data)**2)/sum(is.na(masked_data))), res)
  names(res2) = c('Overall', names(res))
  
  res2
}

absolute_deviation_distro = function(X) {
  
  par(mfrow = c(2,2))
  
  for(i in 2:5) {
    hist(abs(X[,i] - data[,i])[is.na(masked_data[,i])], xlab = 'Absolute Deviation', main = colnames(X)[i], probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  
  par(mfrow = c(1,1))
}

absolute_deviation_distro(X.full.frequentist)


compute_MSE(X.full.frequentist)

```

As expected, this imputation method makes it so that the MSE is about $1$ (significantly more different for the columns with fewer NAs).

## Model 2: it was a Multivariate Normal all along

The first data imputation strategy consists in modelling the data as if coming out of a multivariate normal distribution; the normality assumption on the variables is not really good so this approach is not optimal for this dataset.

Three chains are run in this setup, each with a random set of prior means and variances (distributed, respectively, as a standard Normal and as a Gamma). The prior correlations between the variables are all set to $0$, and the prior missing values are just the column averages.

```{r, echo = FALSE}
set.seed(420)
plan(multisession, workers = 8)

X.full.mvnorm = masked_data


# initialization of output
M = 1000
n_chains = 3
burnin = 200
THETA.rmvnorm = array(0, dim = c(M-burnin, n_chains, dim(X.full.mvnorm)[2]))
SIGMA.rmvnorm = array(0, dim = c(M-burnin, n_chains, dim(X.full.mvnorm)[2]**2))
X.MISS.rmvnorm = array(0, dim = c(M-burnin, n_chains, sum(is.na(X.full.mvnorm))))


### prior parameters
n = dim(X.full.mvnorm)[1]
p = dim(X.full.mvnorm)[2]

mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)
nu0 = p+2
S0 = L0
invL0 = solve(L0)


### starting values
Sigma = S0
X.full = as.matrix(X.full.mvnorm)
O = 1*(!is.na(X.full))

# starting missing values are just column means
for(j in 1:p) {
  X.full[is.na(X.full[,j]), j] = mean(X.full[, j], na.rm=TRUE)
}



populate_missing = function(x, o, theta, Sigma) {
  b = ( o==0 )
  a = ( o==1 )
  
  if(all(a)) {
    return(x)
  }
  iSa = solve( Sigma[a, a] )
  beta.j = Sigma[b,a]%*%iSa
  Sigma.j = Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
  theta.j = theta[b] + beta.j%*%t(t(x[a]) - theta[a])
  x[b] = rmvnorm(1, theta.j, Sigma.j)
  
  return(x)
}



pb = progress_bar$new(total = M)

### Gibbs sampler
THETA = matrix(nrow = M, ncol = dim(X.full.mvnorm)[2])
SIGMA = matrix(nrow = M, ncol = dim(X.full.mvnorm)[2]**2)
X.MISS = matrix(nrow = M, ncol = sum(is.na(X.full.mvnorm)))

for(s in 1:M) {
  
  ###update theta
  Xbar = apply(X.full, 2, mean)
  
  invSigma = solve(Sigma)
  
  Ln = solve(invL0 + n * invSigma)
  mun = Ln%*%( invL0%*%mu0 + n*invSigma%*%Xbar )
  
  theta = rmvnorm(1,mun,Ln)
  
  ###
  ###update Sigma
  
  Sn = S0 + ( t(X.full)-c(theta) )%*%t(t(X.full)-c(theta))
  Sigma = solve(rwish(nu0+n,solve(Sn)))
  
  ###
  ###update missing data
  X.full = t(future_mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma), future.seed=TRUE))
  
  ### save results
  
  THETA[s,] = theta
  SIGMA[s,] = c(Sigma)
  X.MISS[s,] = X.full[O==0]
  
  pb$tick()
  ###
}
###


X.MISS.rmvnorm[,1,] = X.MISS[(burnin+1):M,]
THETA.rmvnorm[,1,] = THETA[(burnin+1):M,]
SIGMA.rmvnorm[,1,] = SIGMA[(burnin+1):M,]

```

```{r, echo = FALSE}

X.full.mvnorm = masked_data


mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)


### prior parameters
n = dim(X.full.mvnorm)[1]
p = dim(X.full.mvnorm)[2]
nu0 = p+2
S0 = L0
invL0 = solve(L0)


### starting values
Sigma = S0
X.full = as.matrix(X.full.mvnorm)
O = 1*(!is.na(X.full))

# starting missing values are just column means
for(j in 1:p) {
  X.full[is.na(X.full[,j]), j] = mean(X.full[, j], na.rm=TRUE)
}



populate_missing = function(x, o, theta, Sigma) {
  b = ( o==0 )
  a = ( o==1 )
  
  if(all(a)) {
    return(x)
  }
  iSa = solve( Sigma[a, a] )
  beta.j = Sigma[b,a]%*%iSa
  Sigma.j = Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
  theta.j = theta[b] + beta.j%*%t(t(x[a]) - theta[a])
  x[b] = rmvnorm(1, theta.j, Sigma.j)
  
  return(x)
}



pb = progress_bar$new(total = M)

### Gibbs sampler
THETA = matrix(nrow = M, ncol = dim(X.full.mvnorm)[2])
SIGMA = matrix(nrow = M, ncol = dim(X.full.mvnorm)[2]**2)
X.MISS = matrix(nrow = M, ncol = sum(is.na(X.full.mvnorm)))

for(s in 1:M) {
  
  ###update theta
  Xbar = apply(X.full, 2, mean)
  
  invSigma = solve(Sigma)
  
  Ln = solve(invL0 + n * invSigma)
  mun = Ln%*%( invL0%*%mu0 + n*invSigma%*%Xbar )
  
  theta = rmvnorm(1,mun,Ln)
  
  ###
  ###update Sigma
  
  Sn = S0 + ( t(X.full)-c(theta) )%*%t(t(X.full)-c(theta))
  Sigma = solve(rwish(nu0+n,solve(Sn)))
  
  ###
  ###update missing data
  X.full = t(future_mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma), future.seed=TRUE))
  
  ### save results
  
  THETA[s,] = theta
  SIGMA[s,] = c(Sigma)
  X.MISS[s,] = X.full[O==0]
  
  pb$tick()
  ###
}
###



X.MISS.rmvnorm[,2,] = X.MISS[(burnin+1):M,]
THETA.rmvnorm[,2,] = THETA[(burnin+1):M,]
SIGMA.rmvnorm[,2,] = SIGMA[(burnin+1):M,]

```



```{r, echo = FALSE}

X.full.mvnorm = masked_data


mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)


### prior parameters
n = dim(X.full.mvnorm)[1]
p = dim(X.full.mvnorm)[2]
nu0 = p+2
S0 = L0
invL0 = solve(L0)


### starting values
Sigma = S0
X.full = as.matrix(X.full.mvnorm)
O = 1*(!is.na(X.full))

# starting missing values are just column means
for(j in 1:p) {
  X.full[is.na(X.full[,j]), j] = mean(X.full[, j], na.rm=TRUE)
}



populate_missing = function(x, o, theta, Sigma) {
  b = ( o==0 )
  a = ( o==1 )
  
  if(all(a)) {
    return(x)
  }
  iSa = solve( Sigma[a, a] )
  beta.j = Sigma[b,a]%*%iSa
  Sigma.j = Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
  theta.j = theta[b] + beta.j%*%t(t(x[a]) - theta[a])
  x[b] = rmvnorm(1, theta.j, Sigma.j)
  
  return(x)
}



pb = progress_bar$new(total = M)

### Gibbs sampler
THETA = matrix(nrow = M, ncol = dim(X.full.mvnorm)[2])
SIGMA = matrix(nrow = M, ncol = dim(X.full.mvnorm)[2]**2)
X.MISS = matrix(nrow = M, ncol = sum(is.na(X.full.mvnorm)))

for(s in 1:M) {
  
  ###update theta
  Xbar = apply(X.full, 2, mean)
  
  invSigma = solve(Sigma)
  
  Ln = solve(invL0 + n * invSigma)
  mun = Ln%*%( invL0%*%mu0 + n*invSigma%*%Xbar )
  
  theta = rmvnorm(1,mun,Ln)
  
  ###
  ###update Sigma
  
  Sn = S0 + ( t(X.full)-c(theta) )%*%t(t(X.full)-c(theta))
  Sigma = solve(rwish(nu0+n,solve(Sn)))
  
  ###
  ###update missing data
  X.full = t(future_mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma), future.seed=TRUE))
  
  ### save results
  
  THETA[s,] = theta
  SIGMA[s,] = c(Sigma)
  X.MISS[s,] = X.full[O==0]
  
  pb$tick()
  ###
}
###



X.MISS.rmvnorm[,3,] = X.MISS[(burnin+1):M,]
THETA.rmvnorm[,3,] = THETA[(burnin+1):M,]
SIGMA.rmvnorm[,3,] = SIGMA[(burnin+1):M,]


```


### Convergence diagnostics
Let's look at some diagnostics. Since we have lots of missing data, we cannot look in detail at each of the chains to look for convergence; we will rely instead on scalar measures of convergence. In particular, here you can see the distribution of the potential scale reduction factor for the 3 set of parameters:

```{r, fig.width = 9, fig.height = 4, echo = FALSE}
manual_gelman = function(X) {
  L = dim(X)[1]
  n_chains = dim(X)[2]
  x_c = apply(X, 3, colMeans)
  x = colMeans(x_c)
  B = L * colSums((x_c-x)**2)/(n_chains-1)
  W = colSums((apply(X, 3, as.vector) - x)**2) / (n_chains*(L-1))
  return((B*(1+1/n_chains) + (L-1)*W)/(L*W))
}

gelman = function(X) {
  return(apply(X, 3, function(x) R.hat(x, burn.in = 0)))
}


compute_PSRF = function(THETA, SIGMA, X.MISS) {
  par(mfrow = c(1,3))
  hist(gelman(THETA), xlab = 'PSRF', main = 'THETA', probability = FALSE, col = somecolors[[6]], border = somecolornames[6])
  hist(gelman(SIGMA), xlab = 'PSRF', main = 'SIGMA', probability = FALSE, col = somecolors[[8]], border = somecolornames[8])
  hist(gelman(X.MISS), xlab = 'PSRF', main = 'X.MISS', probability = FALSE, col = somecolors[[13]], border = somecolornames[13])
  par(mfrow = c(1,1))
}


compute_PSRF(THETA.rmvnorm, SIGMA.rmvnorm, X.MISS.rmvnorm)
```

And here you can see the distribution of the geweke statistic for each chain in each set of parameters:

```{r, fig.width = 11, fig.height = 11, echo = FALSE}

geweke = function(X) {
  return(apply(X, 2, function(x) apply(x, 2, function(x) krige::geweke(as.matrix(x)))[1,]))
}


compute_geweke = function(THETA, SIGMA, X.MISS) {
  par(mfrow = c(3,3))
  
  THETA = geweke(THETA)
  SIGMA = geweke(SIGMA)
  X.MISS = geweke(X.MISS)
  
  for(i in 1:3) {
    hist(THETA[,i], xlab = 'Geweke statistic', main = 'THETA', probability = F, col = somecolors[[5]], border = somecolornames[5])
    
    hist(SIGMA[,i], xlab = 'Geweke statistic', main = 'SIGMA', probability = F, col = somecolors[[8]], border = somecolornames[8])
    
    h = hist(X.MISS[,i], xlab = 'Geweke statistic', main = 'X.MISS', probability = T, col = somecolors[[21]], border = somecolornames[21], ylim = c(0,dnorm(0)))
    curve(dnorm(x), from = h$breaks[1], to = h$breaks[length(h$breaks)], add = T, lwd = 2, col = somecolornames[14])
    
  }
  
  par(mfrow = c(1,1))
}


compute_geweke(THETA.rmvnorm, SIGMA.rmvnorm, X.MISS.rmvnorm)

```
Both statistics look pretty good. On the X.MISS category you can observe a superimposed standard normal; since we computed so many statistics, the significance does not lie in the fact that the values are in $[-1.96, 1.96]$, but rather in verifying that the distribution is not too far from a normal.


### Performance evaluation

We now want to infer the final imputed values to compare them to the other models. To compute these values we use the average of the values extracted for each of the missing points.

```{r, fig.width = 11, fig.height = 10, echo = FALSE}

X.full.mvnorm = masked_data
X.full.mvnorm[is.na(masked_data)] =  apply(X.MISS.rmvnorm, 3, mean)
colnames(X.full.mvnorm) = colnames(masked_data)


pairs(X.full.mvnorm, panel = panel.cor, diag.panel = panel.hist, main = "Model 2")

```

The plots look a lot better than in the frequentist case. The correlations still have some problems but they are much less noticeable.

Let's see how it performs with respect to the "true" masked values.

```{r, fig.width = 6, fig.height = 4, echo = FALSE}

compute_MSE = function(x) {
  cat('MSE of the imputed data wrt the correct ones:\n')
  res = sqrt(colSums((x[,-1] - data[,-1])**2)/colSums(is.na(masked_data[,-1])))
  
  res2 = c(sqrt(sum((x - data)**2)/sum(is.na(masked_data))), res)
  names(res2) = c('Overall', names(res))
  
  res2
}

absolute_deviation_distro = function(X) {
  
  par(mfrow = c(2,2))
  
  for(i in 2:5) {
    hist(abs(X[,i] - data[,i])[is.na(masked_data[,i])], xlab = 'Absolute Deviation', main = colnames(X)[i], probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  
  par(mfrow = c(1,1))
}


absolute_deviation_distro(X.full.mvnorm)


compute_MSE(X.full.mvnorm)

```

This imputation method is a big improvement wrt the frequentist approach in terms of MSE. It's interesting to note that the smallest value does not belong to the class with the smallest number of NA value, marking the statistical basis of the method.


## Model 3: maybe Depth is important after all..

In this third attempt I decided to leverage the fact that the distribution of the features conditional on **Depthm** can be much more reasonably approximated with a gaussian, so this approach actually consists in binning the depth variable and running the previous Multivariate Normal approach independently on each of these bins. This method mimics the use of the conditional distribution of the other variables give **Depthm**.

The bins for this variable have been chosen by hand, selecting an higher density for smaller values and a smaller density for higher values in such a way that the number of samples in each bin would be more or less constant. In this plot you can see how the $25$ bins were chosen:

```{r, fig.width = 8, fig.height = 4, echo = FALSE}

binning_depth = function(levels) {
  levels = levels
  
  l = 0
  
  levels[levels == 0] = 0
  
  tmp = levels>0 & levels <= 105
  levels[tmp] = round(levels[tmp]/10-0.001)*10
  
  tmp = levels>105 & levels <= 245
  levels[tmp] = round((levels[tmp] - 15)/20 - 0.001)*20 + 15
  
  tmp = levels>245 & levels <= 545
  levels[tmp] = round((levels[tmp] - 95)/100 - 0.001)*100 + 95
  
  tmp = levels>545 & levels <= 845
  levels[tmp] = 695
  
  tmp = levels>845 & levels <= 1000
  levels[tmp] = 922
  
  tmp = levels>1000 & levels <= 2000
  levels[tmp] = round((levels[tmp] - 500)/1000 - 0.00001)*1000+500
  
  tmp = levels>2000
  levels[tmp] = 3000
  
  return( levels )
}


# unnormalized depth variable
depth = read.csv('194903-202010_Bottle.csv', sep = ",")
depth = depth[data_idx, 5]
depth = sort(depth)


X.full.binning = data.frame(masked_data)
bin_X = split(X.full.binning[,-1], binning_depth(depth))
n_bins = length(bin_X)
NA_per_bin = unlist(lapply(bin_X, function(x) sum(is.na(x))))
NA_per_bin_per_col = data.matrix(data.frame(bind_rows(lapply(bin_X , function(x) colSums(is.na(x))))))

hist(depth, probability = T, breaks = 50, col = somecolors[[6]], border = somecolornames[6], xlab = 'Depth', main = '', xlim = c(0,3000))
curve(dexp(x, 1/scaled_center[1]), from = 0, to = 5000, col = somecolornames[1], lwd = 2, add = T)

for (val in sort(unique(binning_depth(depth)))) {
  abline(v = val, col="orchid", lwd=1, lty=2)
}

lines(as.numeric(names(table(binning_depth(depth)))), table(binning_depth(depth))/sum(depth), lwd = 2, col = somecolornames[[10]])

par(new = TRUE)
plot(names(NA_per_bin), NA_per_bin, lty = 1,lwd = 1.5, col = somecolornames[[24]], axes = FALSE, xlab = '', ylab = '', ylim = c(0,1000), type = 'l') 
axis(side = 4, at = pretty(range(NA_per_bin)), col = somecolornames[[24]], col.axis = somecolornames[[24]], lwd = 1.5)

legend("topright",   # Position
       inset = 0.05, # Distance from the margin as a fraction of the plot region
       legend = c("Estimated density", "Bins", 'Samples per bin', 'NA per bin'),
       lty = c(1, 2, 1, 1),
       col = c(somecolornames[1], 'orchid', somecolornames[[10]], somecolornames[[24]]),
       lwd = 2)


print('Percentage of NA values for each column per each BIN:')
tmp = cbind(as.numeric(names(bin_X)), NA_per_bin_per_col/unlist(lapply(bin_X, function(x) dim(x)[1]))*100)
tmp = cbind(tmp, unlist(lapply(bin_X, function(x) dim(x)[1])))
colnames(tmp)[1] = 'Depthm'
colnames(tmp)[6] = 'Number of samples'
print(tmp)
```


Let's also have a look at the correlations between the variables inside some of the bins:
```{r, fig.width = 11, fig.height = 10, echo = FALSE}

pairs(bin_X[[1]][apply(!is.na(bin_X[[1]]), 1, all),], panel = panel.cor, diag.panel = panel.hist, main = "Full dataset: depth = 0 m")
pairs(bin_X[[6]][apply(!is.na(bin_X[[6]]), 1, all),], panel = panel.cor, diag.panel = panel.hist, main = "Full dataset: depth = 50 m")
pairs(bin_X[[19]][apply(!is.na(bin_X[[19]]), 1, all),], panel = panel.cor, diag.panel = panel.hist, main = "Full dataset: depth = 295 m")
pairs(bin_X[[24]][apply(!is.na(bin_X[[24]]), 1, all),], panel = panel.cor, diag.panel = panel.hist, main = "Full dataset: depth = 1500 m")

```

This approach is probably not suited for this dataset, given how many NA rows are there in each bin. Some conditional correlations are reasonable, while others are still clearly nonlinear.

Three chains are run in this setup, each with a random set of prior means and variances (distributed, respectively, as a standard Normal and as a Gamma). The prior correlations between the variables are all set to $0$, and the prior missing values are just the column averages.

```{r, echo = FALSE}
set.seed(420)
plan(multisession, workers = 14)

X.full.binning = data.frame(masked_data)
bin_X = split(X.full.binning[,-1], binning_depth(depth))
n_bins = length(bin_X)

# initialization of output
M = 5000
n_chains = 3
burnin = 200
THETA.binning = array(0, dim = c(M-burnin, n_chains, dim(X.full.binning)[2]-1))
SIGMA.binning = array(0, dim = c(M-burnin, n_chains, (dim(X.full.binning)[2]-1)**2))

# create overall results list
res.binning = list()
for (i in 1:n_bins) {
  res.binning[[i]] = list(THETA.binning = THETA.binning, SIGMA.binning = SIGMA.binning, X.MISS.binning = array(0, dim = c(M-burnin, n_chains, NA_per_bin[i])))
}
names(res.binning) = names(bin_X)

### prior parameters
n = dim(X.full.binning)[1]
p = dim(X.full.binning)[2] - 1


mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)


populate_missing = function(x, o, theta, Sigma) {
  b = ( o==0 )
  a = ( o==1 )
  
  if(all(a)) {
    return(x)
  }
  
  iSa = solve( Sigma[a, a] )
  beta.j = Sigma[b,a]%*%iSa
  Sigma.j = Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
  theta.j = theta[b] + beta.j%*%t(t(x[a]) - theta[a])
  x[b] = rmvnorm(1, theta.j, Sigma.j)
  
  return(x)
}


GibbsSample = function(X, mu0, L0, M) {
  n = dim(X)[1]
  p = dim(X)[2]
  
  invL0 = solve(L0)
  nu0 = p+2
  
  ###
  ### starting values
  Sigma = L0
  O = 1*(!is.na(X))
  X.full = as.matrix(X)
  
  # starting missing values are just column means
  for(j in 1:p) {
    X.full[is.na(X.full[,j]), j] = mean(X.full[, j], na.rm=TRUE)
  }
  
  ### Gibbs sampler
  THETA = matrix(nrow = M, ncol = dim(X)[2])
  SIGMA = matrix(nrow = M, ncol = dim(X)[2]**2)
  X.MISS = matrix(nrow = M, ncol = sum(is.na(X)))
  
  for(s in 1:M) {
    
    ###update theta
    Xbar = apply(X.full, 2, mean)
    
    invSigma = solve(Sigma)
    
    Ln = solve(invL0 + n * invSigma)
    mun = Ln%*%( invL0%*%mu0 + n*invSigma%*%Xbar )
    
    theta = rmvnorm(1,mun,Ln)
    
    ###
    ###update Sigma
    
    Sn = L0 + ( t(X.full)-c(theta) )%*%t(t(X.full)-c(theta))
    Sigma = solve(rwish(nu0+n,solve(Sn)))
    
    ###
    ###update missing data
    #X.full = t(future_mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma), future.seed=TRUE))
    X.full = t(mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma)))
    
    ### save results
    
    THETA[s,] = theta
    SIGMA[s,] = c(Sigma)
    X.MISS[s,] = X.full[O==0]
    ###
  }
  
  res = list(THETA = THETA, SIGMA = SIGMA, X.MISS = X.MISS)
  
  return(res)
  ###
  
}


out = future_lapply(bin_X, GibbsSample, mu0 = mu0, L0 = L0, M = M, future.seed=TRUE)

# populate results list
for(i in 1:n_bins) {
  for(j in 1:3) {
    res.binning[[i]][[j]][,1,] = out[[i]][[j]][(burnin+1):M,]
  }
}


```

```{r, echo = FALSE}

X.full.binning = data.frame(masked_data)
bin_X = split(X.full.binning[,-1], binning_depth(depth))


### prior parameters
n = dim(X.full.binning)[1]
p = dim(X.full.binning)[2] - 1


mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)


out = future_lapply(bin_X, GibbsSample, mu0 = mu0, L0 = L0, M = M, future.seed=TRUE)

# populate results list
for(i in 1:n_bins) {
  for(j in 1:3) {
    res.binning[[i]][[j]][,2,] = out[[i]][[j]][(burnin+1):M,]
  }
}

```

```{r, echo = FALSE}

X.full.binning = data.frame(masked_data)
bin_X = split(X.full.binning[,-1], binning_depth(depth))


### prior parameters
n = dim(X.full.binning)[1]
p = dim(X.full.binning)[2] - 1


mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)


out = future_lapply(bin_X, GibbsSample, mu0 = mu0, L0 = L0, M = M, future.seed=TRUE)

# populate results list
for(i in 1:n_bins) {
  for(j in 1:3) {
    res.binning[[i]][[j]][,3,] = out[[i]][[j]][(burnin+1):M,]
  }
}


```


### Convergence diagnostics
Let's look at some diagnostics. Here you can see the distribution of the potential scale reduction factor for the 3 set of parameters in each chain:

```{r, fig.width = 12, fig.height = 13, echo = FALSE}

compute_PSRF_bins = function(res) {
  pre = par()$oma
  par(mfrow = c(5,5), oma = c(0,0,3,0))
  for (i in 1:n_bins) {
    hist(gelman(res[[i]][[1]]), xlab = 'PSRF', main = paste('Depth = ',names(res)[i]), probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  mtext("THETA", side = 3, line = 1, cex = 1.2, font = 2, outer = TRUE)
  
  for (i in 1:n_bins) {
    hist(gelman(res[[i]][[2]]), xlab = 'PSRF', main = paste('Depth = ',names(res)[i]), probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  mtext("SIGMA", side = 3, line = 1, cex = 1.2, font = 2, outer = TRUE)
  
  for (i in 1:n_bins) {
    hist(gelman(res[[i]][[3]]), xlab = 'PSRF', main = paste('Depth = ',names(res)[i]), probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  mtext("X.MISS", side = 3, line = 1, cex = 1.2, font = 2, outer = TRUE)
  
  par(mfrow = c(1,1), oma = pre)
}


compute_PSRF_bins(res.binning)
```
The chain corresponding to the THETA has good indication of convergence; while the chain corresponding to the Sigma has troubles converging, let's look more in detail at one of them: $659\ m$ of depth. We can easily see the cause of the big reduction factor: the three chains are completely off.

```{r, fig.width = 6, fig.height = 3, echo = FALSE}

print('PSRF for Depth = 695 m')
print(matrix(gelman(res.binning[[22]][[2]]), ncol = 4))

print('Means of the 3 chains for the variance of the O2Concentration:')
print(colMeans((res.binning[[22]][[2]][,,6])))

```
This particular problem is due to the inadequacy of the model: the bin corresponding to $295 m$ still has highly nonlinear correlations.

And here you can see the distribution of the geweke statistic for each chain in each set of parameters. I decided to join the values for the three chains to make it more readable:

```{r, fig.width = 12, fig.height = 13, echo = FALSE}

compute_geweke_bins = function(res) {
  
  pre = par()$oma
  par(mfrow = c(5,5), oma = c(0,0,3,0))
  
  for (i in 1:n_bins) {
    THETA = geweke(res[[i]][[1]])
    
    hist(THETA, xlab = 'Geweke statistic', main = paste('Depth = ',names(res)[i]), probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  mtext("THETA", side = 3, line = 1, cex = 1.2, font = 2, outer = TRUE)
  
  for (i in 1:n_bins) {
    SIGMA = geweke(res[[i]][[2]])
    
    hist(SIGMA, xlab = 'Geweke statistic', main = paste('Depth = ',names(res)[i]), probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  mtext("SIGMA", side = 3, line = 1, cex = 1.2, font = 2, outer = TRUE)
  
  for (i in 1:n_bins) {
    X.MISS = geweke(res[[i]][[3]])
    
    h = hist(X.MISS, xlab = 'Geweke statistic', main = paste('Depth = ',names(res)[i]), probability = TRUE, col = somecolors[[i]], border = somecolornames[i], ylim = c(0,dnorm(0)))
    
    curve(dnorm(x), from = h$breaks[1], to = h$breaks[length(h$breaks)], add = T, lwd = 2, col = somecolornames[(i+1)%%25+1])
  }
  mtext("X.MISS", side = 3, line = 1, cex = 1.2, font = 2, outer = TRUE)
  
  par(mfrow = c(1,1), oma = pre)
}


compute_geweke_bins(res.binning)

```

The values for the parameters here are pretty good, so the individual chains probably reached a stable points; there are clearly problems with the model.


### Performance evaluation

We now want to infer the final imputed values to compare them to the other models. To compute these values we use the average of the values extracted for each of the missing points.

```{r, fig.width = 11, fig.height = 10, echo = FALSE}

X.full.binning = data.frame(masked_data)


# use expectation
X.full.binning[is.na(X.full.binning)] = unlist(lapply(res.binning, function(x) apply(x[[3]], 3, mean)), use.names = FALSE)
colnames(X.full.binning) = colnames(masked_data)


pairs(X.full.binning, panel = panel.cor, diag.panel = panel.hist, main = "Model 3")

```

The individual distributions are pretty close, but the correlations between the variables are really off. It's peculiar to see how the two most linearly correlated variables have messed up results in their correlations.

Let's see how it performs with respect to the "true" masked values.

```{r, fig.width = 8, fig.height = 6, echo = FALSE}

compute_MSE = function(x) {
  cat('MSE of the imputed data wrt the correct ones:\n')
  res = sqrt(colSums((x[,-1] - data[,-1])**2)/colSums(is.na(masked_data[,-1])))
  
  res2 = c(sqrt(sum((x - data)**2)/sum(is.na(masked_data))), res)
  names(res2) = c('Overall', names(res))
  
  res2
}

absolute_deviation_distro = function(X) {
  
  par(mfrow = c(2,2))
  
  for(i in 2:5) {
    hist(abs(X[,i] - data[,i])[is.na(masked_data[,i])], xlab = 'Absolute Deviation', main = colnames(X)[i], probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  
  par(mfrow = c(1,1))
}


compute_MSE_per_col = function(X) {
  bin_MSE = split(data.frame((X[,-1]-data[,-1])**2), binning_depth(depth))

  Overall = unlist(lapply(bin_MSE, function(x) sqrt(sum(x))))/sqrt(NA_per_bin)
    
  res = data.matrix(bind_rows(lapply(bin_MSE, function(x) sqrt(colSums(x)))))/sqrt(NA_per_bin_per_col)
  
  res = cbind(res, Overall)
  res = cbind(as.numeric(names(bin_MSE)), res)
  
  plot(res[!is.na(res[,2]),1],res[!is.na(res[,2]),2], ylim = c(min(res[,-1], na.rm = T), max(res[,-1], na.rm = T)), type = 'l', lwd = 2, col = somecolornames[[2]], ylab = 'MSE', xlab = 'Depth')
  for (i in 3:6) {
    lines(res[!is.na(res[,i]),1],res[!is.na(res[,i]),i], lwd = 2, col = somecolornames[[i]])
  }
  abline(1,0, lty = 2, lwd = 2)
  grid()
  
  legend("topright",   # Position
         inset = 0.05, # Distance from the margin as a fraction of the plot region
         legend = colnames(res)[-1],
         lty = 1,
         col = somecolornames[2:6],
         lwd = 2)
}





absolute_deviation_distro(X.full.binning)


compute_MSE_per_col(X.full.binning)


compute_MSE(X.full.binning)


```

As we can see, the MSE is worse than the frequentist case! This is probably due to the big number of NA values wrt the total number of data points in each bin.


## Model 4: a good old regression

As a final attempt, we remarked before how the correlation between **Depthm** and the other variables was quite clear, let's plot them here:

```{r, fig.width = 8, fig.height = 8, echo = FALSE}

par(mfrow = c(2,2))

for(i in 2:5) {
  plot(masked_data[,1], masked_data[,i], col = somecolors[[i]], pch = 20, ylab = '', main = colnames(masked_data)[i], xlab = 'Depth')
}

par(mfrow = c(1,1))

```


These plots remind of some basic functional forms of the type:
$$
A(1-e^{-Bx}) + C\qquad\qquad\qquad -Axe^{-Bx}+C
$$
with parameters $A,B,C$, the idea is then to infer each missing point only using the **Depthm** column by fitting the above functional forms to the data.

Here is an example of this two functional shapes for a suitable set of parameters $A,B,C$:
```{r, fig.width = 8, fig.height = 5, echo = FALSE}

curve((function(x) 10*(1-exp(-0.2*x)))(x), from = -2, to = 30, ylim = c(-6,10), lwd = 2, col = somecolornames[[3]], main = '', ylab = '', xlab = 'x')

curve((function(x) -3*x*exp(-0.2*x))(x), from = -2, to = 30, col = somecolornames[[4]], lwd = 2, add = T)

grid()

legend("bottomright",   # Position
       inset = 0.05, # Distance from the margin as a fraction of the plot region
       legend = c(TeX('$10(1-e^{-0.2x})$'), TeX('$-3xe^{-0.2x}$')),
       lty = 1,
       col = c(somecolornames[[3]], somecolornames[[4]]),
       lwd = 2,
       cex = 2)

```


For each column three chains are run, each with a random set of prior $A,B,C$. Let's start with the temperature column, we will cheat and look at the plots to infer the most suitable prior distribution for the parameters. We need:
* $A$ to be negative (as the function should decrease), so we set $-A\sim Gamma(1,1)$
* $B$ to be positive, so we set $B\sim Gamma(1,1)$
* $C$ to be positive, so we set $C\sim Gamma(1,1)$

The Jags model used is:

```{r}
TempReg = "
  model {
    #Likelihood
    for (i in 1:n) {
      y[i]~dnorm(mu[i],tau)
      mu[i] <- A*(1-exp(-B*X[i])) + C
    }
    
    #Priors
    Aneg ~ dgamma(1,1)
    B ~ dgamma(1,1)
    C ~ dgamma(1,1)
    A = -Aneg
    tau <- 1 / (sigma * sigma)
    sigma~dunif(0,100)
  }
"
```


Three parallel chains are used for this model, with thinning of 10 points:

```{r, echo = FALSE, message=FALSE, results='hide'}
set.seed(420)

X.full.regression = masked_data


depth = X.full.regression[,1]
temp = X.full.regression[,2]
O2con = X.full.regression[,3]
STheta = X.full.regression[,4]
O2sat = X.full.regression[,5]


# initialization of output
M = 3000
n_chains = 3
burnin = 200

dd <- list("X" = depth, 'y' = temp, "n" = n)

params <- c("A", 'B', 'C', 'sigma')


jags_temp <- jags(data=dd,
                    parameters.to.save=params,
                    model.file=textConnection(TempReg),
                    n.chains=n_chains,
                    n.iter=M,
                    jags.seed = 420,
                    n.burnin=burnin,
                    n.thin=10)


```

Similarly, for **STheta** we want:
* $A$ to be negative (as the function should increase), so we set $A\sim Gamma(1,1)$
* $B$ to be positive, so we set $B\sim Gamma(1,1)$
* $C$ to be negative, so we set $-C\sim Gamma(1,1)$

The Jags model used is:

```{r}
SThetaReg = "
  model {
    #Likelihood
    for (i in 1:n) {
      y[i]~dnorm(mu[i],tau)
      mu[i] <- A*(1-exp(-B*X[i])) + C
    }
    
    #Priors
    A ~ dgamma(1,1)
    B ~ dgamma(1,1)
    Cneg ~ dgamma(1,1)
    C=-Cneg
    tau <- 1 / (sigma * sigma)
    sigma~dunif(0,100)
  }
"
```


```{r, echo = FALSE, message=FALSE, results='hide'}
dd <- list("X" = depth, 'y' = STheta, "n" = n)

params <- c("A", 'B', 'C', 'sigma')


jags_STheta <- jags(data=dd,
                    parameters.to.save=params,
                    model.file=textConnection(SThetaReg),
                    n.chains=n_chains,
                    n.iter=M,
                    jags.seed = 420,
                    n.burnin=burnin,
                    n.thin=10)
```


For **O2con** we want:
* $A$ to be positive (as the function should decrease), so we set $A\sim Gamma(1,1)$
* $B$ to be positive, so we set $B\sim Gamma(1,1)$
* $C$ to be negative, so we set $-C\sim Gamma(1,1)$

The Jags model used is:


```{r}
O2conReg = "
  model {
    #Likelihood
    for (i in 1:n) {
      y[i]~dnorm(mu[i],tau)
      mu[i] <- -A*X[i] * exp(-B*X[i]) + C
    }
    
    #Priors
    A ~ dgamma(1,1)
    B ~ dgamma(1,1)
    Cneg ~ dgamma(1,1)
    C = -Cneg
    tau <- 1 / (sigma * sigma)
    sigma~dunif(0,100)
  }
"
```


```{r, echo = FALSE, message=FALSE, results='hide'}
dd <- list("X" = depth, 'y' = O2con, "n" = n)

params <- c("A", 'B', 'C', 'sigma')


jags_O2con <- jags(data=dd,
                    parameters.to.save=params,
                    model.file=textConnection(O2conReg),
                    n.chains=n_chains,
                    n.iter=M,
                    jags.seed = 420,
                    n.burnin=burnin,
                   n.thin=10)
```

Similarly, for **O2Sat** we want:
* $A$ to be positive (as the function should decrease), so we set $A\sim Gamma(1,1)$
* $B$ to be positive, so we set $B\sim Gamma(1,1)$
* $C$ to be negative, so we set $-C\sim Gamma(1,1)$

The Jags model used is:

```{r}
O2satReg = "
  model {
    #Likelihood
    for (i in 1:n) {
      y[i]~dnorm(mu[i],tau)
      mu[i] <- -A*X[i] * exp(-B*X[i]) + C
    }
    
    #Priors
    A ~ dgamma(1,1)
    B ~ dgamma(1,1)
    Cneg ~ dgamma(1,1)
    C = -Cneg
    tau <- 1 / (sigma * sigma)
    sigma~dunif(0,100)
  }
"
```


```{r, echo = FALSE, message=FALSE, results='hide'}
dd <- list("X" = depth, 'y' = O2sat, "n" = n)

params <- c("A", 'B', 'C', 'sigma')


jags_O2sat <- jags(data=dd,
                    parameters.to.save=params,
                    model.file=textConnection(O2satReg),
                    n.chains=n_chains,
                    n.iter=M,
                    jags.seed = 420,
                    n.burnin=burnin,
                   n.thin=10)
```


Here we plot the inferred curves over the train data to check if something went wrong:

```{r, fig.width = 8, fig.height = 8, echo = FALSE}

plan(multisession, workers = 8)

.linear_pred = function(x, sim_mat) {
  
  n_samples = dim(sim_mat)[1]
  
  means = sim_mat[,'A'] * (1-exp(-sim_mat[,'B']*x)) + sim_mat[,'C']
  
  for (i in 1:n_samples) {
    means[i] = rnorm(1, mean = means[i], sd = sim_mat[i,'sigma'])
  }
  
  return(mean(means))
}

linear_predict = function(X,sim_mat) {
  y = future_sapply(X, .linear_pred, sim_mat = sim_mat, future.seed = TRUE)
  
  return(y)
}

.oxygen_pred = function(x, sim_mat) {
  
  n_samples = dim(sim_mat)[1]
  
  means = -sim_mat[,'A'] * x * exp(-sim_mat[,'B']*x) + sim_mat[,'C']
  
  for (i in 1:n_samples) {
    means[i] = rnorm(1, mean = means[i], sd = sim_mat[i,'sigma'])
  }
  
  return(mean(means))
}

oxygen_predict = function(X,sim_mat) {
  y = future_sapply(X, .oxygen_pred, sim_mat = sim_mat, future.seed = TRUE)
  
  return(y)
}



par(mfrow = c(2,2))

plot(masked_data[,1], masked_data[,2], col = somecolors[[2]], pch = 20, ylab = '', main = colnames(masked_data)[2], xlab = 'Depth')
lines(data[,1], linear_predict(data[,1], jags_temp$BUGSoutput$sims.matrix), col = somecolornames[[3]], lwd = 2)

plot(masked_data[,1], masked_data[,3], col = somecolors[[3]], pch = 20, ylab = '', main = colnames(masked_data)[3], xlab = 'Depth')
lines(data[,1], oxygen_predict(data[,1], jags_O2con$BUGSoutput$sims.matrix), col = somecolornames[[4]], lwd = 2)

plot(masked_data[,1], masked_data[,4], col = somecolors[[4]], pch = 20, ylab = '', main = colnames(masked_data)[4], xlab = 'Depth')
lines(data[,1], linear_predict(data[,1], jags_STheta$BUGSoutput$sims.matrix), col = somecolornames[[5]], lwd = 2)

plot(masked_data[,1], masked_data[,5], col = somecolors[[5]], pch = 20, ylab = '', main = colnames(masked_data)[5], xlab = 'Depth')
lines(data[,1], oxygen_predict(data[,1], jags_O2sat$BUGSoutput$sims.matrix), col = somecolornames[[6]], lwd = 2)



par(mfrow = c(1,1))


```

The fit was successful. A relationship of the type $\frac{A}{x} + B$ would probably suit more temperature and density. The oxygen variables were fitted pretty well. The main bottleneck here is the fact that the variance is constant, if we could have taken into account a functional form for the variance the regression would probably be a bit better.


### Convergence diagnostics
Let's look at some diagnostics. Since we have a small set of parameters here, we can look a bit more in detail for convergence:

```{r, fig.width = 8, fig.height = 8, echo = FALSE}

geweke = function(X) {
  return(apply(X, 2, function(x) apply(x, 2, function(x) krige::geweke(as.matrix(x)))[1,]))
}

gelman = function(X) {
  return(apply(X, 3, function(x) R.hat(x, burn.in = 0)))
}

jags_temp.mcmc = as.mcmc(jags_temp)
out = ggs(jags_temp.mcmc)
ggs_density(out)
ggs_traceplot(out)
ggs_autocorrelation(out)
ggs_running(out)

cat('Potential Scale Reduction factor for the parameters:\n')
gelman(jags_temp$BUGSoutput$sims.array)
cat('\nGeweke statistics for the three chains:\n')
geweke(jags_temp$BUGSoutput$sims.array)
```

```{r, fig.width = 8, fig.height = 8, echo = FALSE}
jags_STheta.mcmc = as.mcmc(jags_STheta)
out = ggs(jags_STheta.mcmc)
ggs_density(out)
ggs_traceplot(out)
ggs_autocorrelation(out)
ggs_running(out)

cat('Potential Scale Reduction factor for the parameters:\n')
gelman(jags_STheta$BUGSoutput$sims.array)
cat('\nGeweke statistics for the three chains:\n')
geweke(jags_STheta$BUGSoutput$sims.array)
```


```{r, fig.width = 8, fig.height = 8, echo = FALSE}
jags_O2con.mcmc = as.mcmc(jags_O2con)
out = ggs(jags_O2con.mcmc)
ggs_density(out)
ggs_traceplot(out)
ggs_autocorrelation(out)
ggs_running(out)

cat('Potential Scale Reduction factor for the parameters:\n')
gelman(jags_O2con$BUGSoutput$sims.array)
cat('\nGeweke statistics for the three chains:\n')
geweke(jags_O2con$BUGSoutput$sims.array)
```


```{r, fig.width = 8, fig.height = 8, echo = FALSE}
jags_O2sat.mcmc = as.mcmc(jags_O2sat)
out = ggs(jags_O2sat.mcmc)
ggs_density(out)
ggs_traceplot(out)
ggs_autocorrelation(out)
ggs_running(out)

cat('Potential Scale Reduction factor for the parameters:\n')
gelman(jags_O2sat$BUGSoutput$sims.array)
cat('\nGeweke statistics for the three chains:\n')
geweke(jags_O2sat$BUGSoutput$sims.array)
```


The chains seem to have reached convergence. They would probably benefit from a longer run to eliminate some of the effects we see here.


### Performance evaluation

We now want to infer the final imputed values to compare them to the other models. To compute these values we use the average of the values extracted for each of the missing points.

```{r, fig.width = 11, fig.height = 10, echo = FALSE}

X.full.regression = masked_data

X.full.regression[is.na(masked_data[,2]), 2] = linear_predict(depth[is.na(masked_data[,2])], jags_temp$BUGSoutput$sims.matrix)

X.full.regression[is.na(masked_data[,3]), 3] = oxygen_predict(depth[is.na(masked_data[,3])], jags_O2con$BUGSoutput$sims.matrix)

X.full.regression[is.na(masked_data[,4]), 4] = linear_predict(depth[is.na(masked_data[,4])], jags_STheta$BUGSoutput$sims.matrix)

X.full.regression[is.na(masked_data[,5]), 5] = oxygen_predict(depth[is.na(masked_data[,5])], jags_O2sat$BUGSoutput$sims.matrix)

colnames(X.full.regression) = colnames(masked_data)


pairs(X.full.regression, panel = panel.cor, diag.panel = panel.hist, main = "Model 4")

```

Visually, both the distributions and the correlations are pretty good. Let's see how it performs with respect to the "true" masked values.

```{r, fig.width = 8, fig.height = 7, echo = FALSE}

compute_MSE = function(x) {
  cat('MSE of the imputed data wrt the correct ones:\n')
  res = sqrt(colSums((x[,-1] - data[,-1])**2)/colSums(is.na(masked_data[,-1])))
  
  res2 = c(sqrt(sum((x - data)**2)/sum(is.na(masked_data))), res)
  names(res2) = c('Overall', names(res))
  
  res2
}

absolute_deviation_distro = function(X) {
  
  par(mfrow = c(2,2))
  
  for(i in 2:5) {
    hist(abs(X[,i] - data[,i])[is.na(masked_data[,i])], xlab = 'Absolute Deviation', main = colnames(X)[i], probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  
  par(mfrow = c(1,1))
}


absolute_deviation_distro(X.full.regression)


compute_MSE(X.full.regression)

```

This imputation method is a big improvement wrt the frequentist approach in terms of MSE. It is not quite as good as the stock multivariate normal, but it gets close. Probably a combination of these methods could produce even better results.


## Model 5: Fusion! (knitted in Part 2)

Following the line of the last model, let's fit a simpler model with approximate fixed parameters. The model I chose is:
$$
\frac{A}{x+1} + C
$$
The main advantage is that it has a vertical asymptote, so it seems to suit well this data, and it only has two parameters, so it's much easier to tune. I used this model for all the 4 columns, since the previous oxygen model was not invertible. This is how the handtuned curve looks like:

```{r, fig.width = 8, fig.height = 8, echo = FALSE}

par(mfrow = c(2,2))

plot(masked_data[,1], masked_data[,2], col = somecolors[[2]], pch = 20, ylab = '', main = colnames(masked_data)[2], xlab = 'Depth')
lines(data[,1], 2/(data[,1] + 1) + min(data[,2]) - 2/10, col = somecolornames[[3]], lwd = 2)

plot(masked_data[,1], masked_data[,3], col = somecolors[[3]], pch = 20, ylab = '', main = colnames(masked_data)[3], xlab = 'Depth')
lines(data[,1], 2/(data[,1] + 1) + min(data[,3]) - 2/10, col = somecolornames[[4]], lwd = 2)

plot(masked_data[,1], masked_data[,4], col = somecolors[[4]], pch = 20, ylab = '', main = colnames(masked_data)[4], xlab = 'Depth')
lines(data[,1], -2/(data[,1] + 1) + max(data[,4]) + 2/10, col = somecolornames[[5]], lwd = 2)

plot(masked_data[,1], masked_data[,5], col = somecolors[[5]], pch = 20, ylab = '', main = colnames(masked_data)[5], xlab = 'Depth')
lines(data[,1], 2/(data[,1] + 1) + min(data[,5]) - 2/10, col = somecolornames[[6]], lwd = 2)



par(mfrow = c(1,1))


```

Using this functional forms, I transformed the data to linearize the features, this is the final pairplot:
```{r, echo = FALSE, fig.height=10, fig.width=11}

X.full.normregOR = masked_data

X.full.normregOR[,2] = 2/(X.full.normregOR[,2] + 2/10 - min(X.full.normregOR[,2], na.rm = T)) - 1
X.full.normregOR[,3] = 2/(X.full.normregOR[,3] + 2/10 - min(X.full.normregOR[,3], na.rm = T)) - 1
X.full.normregOR[,4] = -2/(X.full.normregOR[,4] - 2/10 - max(X.full.normregOR[,4], na.rm = T)) - 1
X.full.normregOR[,5] = 2/(X.full.normregOR[,5] + 2/10 - min(X.full.normregOR[,5], na.rm = T)) - 1


scaled_scale_normregOR = scale(X.full.normregOR)
X.full.normregOR = data.frame(scaled_scale_normregOR)
scaled_center_normregOR = attr(scaled_scale_normregOR, 'scaled:center')
scaled_scale_normregOR = attr(scaled_scale_normregOR, 'scaled:scale')


pairs(X.full.normregOR[apply(!is.na(X.full.normregOR), 1, all),], panel = panel.cor, diag.panel = panel.hist, main = "Full dataset")

```

It looks a bit better, but many correlations are still heavily nonlinear. Let's run the multivariate normal strategy on this new dataset.

```{r, echo = FALSE}
set.seed(420)
plan(multisession, workers = 8)


X.full.normreg = X.full.normregOR


# initialization of output
M = 1000
n_chains = 3
burnin = 200
THETA.rnormreg = array(0, dim = c(M-burnin, n_chains, dim(X.full.normreg)[2]))
SIGMA.rnormreg = array(0, dim = c(M-burnin, n_chains, dim(X.full.normreg)[2]**2))
X.MISS.rnormreg = array(0, dim = c(M-burnin, n_chains, sum(is.na(X.full.normreg))))


### prior parameters
n = dim(X.full.normreg)[1]
p = dim(X.full.normreg)[2]

mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)
nu0 = p+2
S0 = L0
invL0 = solve(L0)


### starting values
Sigma = S0
X.full = as.matrix(X.full.normreg)
O = 1*(!is.na(X.full))

# starting missing values are just column means
for(j in 1:p) {
  X.full[is.na(X.full[,j]), j] = mean(X.full[, j], na.rm=TRUE)
}



populate_missing = function(x, o, theta, Sigma) {
  b = ( o==0 )
  a = ( o==1 )
  
  if(all(a)) {
    return(x)
  }
  iSa = solve( Sigma[a, a] )
  beta.j = Sigma[b,a]%*%iSa
  Sigma.j = Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
  theta.j = theta[b] + beta.j%*%t(t(x[a]) - theta[a])
  x[b] = rmvnorm(1, theta.j, Sigma.j)
  
  return(x)
}



pb = progress_bar$new(total = M)

### Gibbs sampler
THETA = matrix(nrow = M, ncol = dim(X.full.normreg)[2])
SIGMA = matrix(nrow = M, ncol = dim(X.full.normreg)[2]**2)
X.MISS = matrix(nrow = M, ncol = sum(is.na(X.full.normreg)))

for(s in 1:M) {
  
  ###update theta
  Xbar = apply(X.full, 2, mean)
  
  invSigma = solve(Sigma)
  
  Ln = solve(invL0 + n * invSigma)
  mun = Ln%*%( invL0%*%mu0 + n*invSigma%*%Xbar )
  
  theta = rmvnorm(1,mun,Ln)
  
  ###
  ###update Sigma
  
  Sn = S0 + ( t(X.full)-c(theta) )%*%t(t(X.full)-c(theta))
  Sigma = solve(rwish(nu0+n,solve(Sn)))
  
  ###
  ###update missing data
  X.full = t(future_mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma), future.seed=TRUE))
  
  ### save results
  
  THETA[s,] = theta
  SIGMA[s,] = c(Sigma)
  X.MISS[s,] = X.full[O==0]
  
  pb$tick()
  ###
}
###


X.MISS.rnormreg[,1,] = X.MISS[(burnin+1):M,]
THETA.rnormreg[,1,] = THETA[(burnin+1):M,]
SIGMA.rnormreg[,1,] = SIGMA[(burnin+1):M,]

```

```{r, echo = FALSE}

X.full.normreg = X.full.normregOR

mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)


### prior parameters
n = dim(X.full.normreg)[1]
p = dim(X.full.normreg)[2]
nu0 = p+2
S0 = L0
invL0 = solve(L0)


### starting values
Sigma = S0
X.full = as.matrix(X.full.normreg)
O = 1*(!is.na(X.full))

# starting missing values are just column means
for(j in 1:p) {
  X.full[is.na(X.full[,j]), j] = mean(X.full[, j], na.rm=TRUE)
}



populate_missing = function(x, o, theta, Sigma) {
  b = ( o==0 )
  a = ( o==1 )
  
  if(all(a)) {
    return(x)
  }
  iSa = solve( Sigma[a, a] )
  beta.j = Sigma[b,a]%*%iSa
  Sigma.j = Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
  theta.j = theta[b] + beta.j%*%t(t(x[a]) - theta[a])
  x[b] = rmvnorm(1, theta.j, Sigma.j)
  
  return(x)
}



pb = progress_bar$new(total = M)

### Gibbs sampler
THETA = matrix(nrow = M, ncol = dim(X.full.normreg)[2])
SIGMA = matrix(nrow = M, ncol = dim(X.full.normreg)[2]**2)
X.MISS = matrix(nrow = M, ncol = sum(is.na(X.full.normreg)))

for(s in 1:M) {
  
  ###update theta
  Xbar = apply(X.full, 2, mean)
  
  invSigma = solve(Sigma)
  
  Ln = solve(invL0 + n * invSigma)
  mun = Ln%*%( invL0%*%mu0 + n*invSigma%*%Xbar )
  
  theta = rmvnorm(1,mun,Ln)
  
  ###
  ###update Sigma
  
  Sn = S0 + ( t(X.full)-c(theta) )%*%t(t(X.full)-c(theta))
  Sigma = solve(rwish(nu0+n,solve(Sn)))
  
  ###
  ###update missing data
  X.full = t(future_mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma), future.seed=TRUE))
  
  ### save results
  
  THETA[s,] = theta
  SIGMA[s,] = c(Sigma)
  X.MISS[s,] = X.full[O==0]
  
  pb$tick()
  ###
}
###



X.MISS.rnormreg[,2,] = X.MISS[(burnin+1):M,]
THETA.rnormreg[,2,] = THETA[(burnin+1):M,]
SIGMA.rnormreg[,2,] = SIGMA[(burnin+1):M,]

```



```{r, echo = FALSE}

X.full.normreg = X.full.normregOR

mu0 = rnorm(p, 0, 1)
sd0 = rgamma(p,3) + 1/10

# 0 prior correlation
L0 = matrix (0, p, p)
diag(L0) = 1
L0 = L0 * outer(sd0,sd0)


### prior parameters
n = dim(X.full.normreg)[1]
p = dim(X.full.normreg)[2]
nu0 = p+2
S0 = L0
invL0 = solve(L0)


### starting values
Sigma = S0
X.full = as.matrix(X.full.normreg)
O = 1*(!is.na(X.full))

# starting missing values are just column means
for(j in 1:p) {
  X.full[is.na(X.full[,j]), j] = mean(X.full[, j], na.rm=TRUE)
}



populate_missing = function(x, o, theta, Sigma) {
  b = ( o==0 )
  a = ( o==1 )
  
  if(all(a)) {
    return(x)
  }
  iSa = solve( Sigma[a, a] )
  beta.j = Sigma[b,a]%*%iSa
  Sigma.j = Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
  theta.j = theta[b] + beta.j%*%t(t(x[a]) - theta[a])
  x[b] = rmvnorm(1, theta.j, Sigma.j)
  
  return(x)
}



pb = progress_bar$new(total = M)

### Gibbs sampler
THETA = matrix(nrow = M, ncol = dim(X.full.normreg)[2])
SIGMA = matrix(nrow = M, ncol = dim(X.full.normreg)[2]**2)
X.MISS = matrix(nrow = M, ncol = sum(is.na(X.full.normreg)))

for(s in 1:M) {
  
  ###update theta
  Xbar = apply(X.full, 2, mean)
  
  invSigma = solve(Sigma)
  
  Ln = solve(invL0 + n * invSigma)
  mun = Ln%*%( invL0%*%mu0 + n*invSigma%*%Xbar )
  
  theta = rmvnorm(1,mun,Ln)
  
  ###
  ###update Sigma
  
  Sn = S0 + ( t(X.full)-c(theta) )%*%t(t(X.full)-c(theta))
  Sigma = solve(rwish(nu0+n,solve(Sn)))
  
  ###
  ###update missing data
  X.full = t(future_mapply(populate_missing, split(X.full, row(X.full)), split(O, row(O)), MoreArgs = list(theta = theta, Sigma = Sigma), future.seed=TRUE))
  
  ### save results
  
  THETA[s,] = theta
  SIGMA[s,] = c(Sigma)
  X.MISS[s,] = X.full[O==0]
  
  pb$tick()
  ###
}
###



X.MISS.rnormreg[,3,] = X.MISS[(burnin+1):M,]
THETA.rnormreg[,3,] = THETA[(burnin+1):M,]
SIGMA.rnormreg[,3,] = SIGMA[(burnin+1):M,]


```


### Convergence diagnostics
Here you can see the distribution of the potential scale reduction factor for the 3 set of parameters:

```{r, fig.width = 9, fig.height = 4, echo = FALSE}
manual_gelman = function(X) {
  L = dim(X)[1]
  n_chains = dim(X)[2]
  x_c = apply(X, 3, colMeans)
  x = colMeans(x_c)
  B = L * colSums((x_c-x)**2)/(n_chains-1)
  W = colSums((apply(X, 3, as.vector) - x)**2) / (n_chains*(L-1))
  return((B*(1+1/n_chains) + (L-1)*W)/(L*W))
}

gelman = function(X) {
  return(apply(X, 3, function(x) R.hat(x, burn.in = 0)))
}


compute_PSRF = function(THETA, SIGMA, X.MISS) {
  par(mfrow = c(1,3))
  hist(gelman(THETA), xlab = 'PSRF', main = 'THETA', probability = FALSE, col = somecolors[[6]], border = somecolornames[6])
  hist(gelman(SIGMA), xlab = 'PSRF', main = 'SIGMA', probability = FALSE, col = somecolors[[8]], border = somecolornames[8])
  hist(gelman(X.MISS), xlab = 'PSRF', main = 'X.MISS', probability = FALSE, col = somecolors[[13]], border = somecolornames[13])
  par(mfrow = c(1,1))
}


compute_PSRF(THETA.rnormreg, SIGMA.rnormreg, X.MISS.rnormreg)
```

And here you can see the distribution of the geweke statistic for each chain in each set of parameters:

```{r, fig.width = 11, fig.height = 11, echo = FALSE}

geweke = function(X) {
  return(apply(X, 2, function(x) apply(x, 2, function(x) krige::geweke(as.matrix(x)))[1,]))
}


compute_geweke = function(THETA, SIGMA, X.MISS) {
  par(mfrow = c(3,3))
  
  THETA = geweke(THETA)
  SIGMA = geweke(SIGMA)
  X.MISS = geweke(X.MISS)
  
  for(i in 1:3) {
    hist(THETA[,i], xlab = 'Geweke statistic', main = 'THETA', probability = F, col = somecolors[[5]], border = somecolornames[5])
    
    hist(SIGMA[,i], xlab = 'Geweke statistic', main = 'SIGMA', probability = F, col = somecolors[[8]], border = somecolornames[8])
    
    h = hist(X.MISS[,i], xlab = 'Geweke statistic', main = 'X.MISS', probability = T, col = somecolors[[21]], border = somecolornames[21], ylim = c(0,dnorm(0)))
    curve(dnorm(x), from = h$breaks[1], to = h$breaks[length(h$breaks)], add = T, lwd = 2, col = somecolornames[14])
    
  }
  
  par(mfrow = c(1,1))
}


compute_geweke(THETA.rnormreg, SIGMA.rnormreg, X.MISS.rnormreg)

```
There does not seem to be any indication of something not converging.


### Performance evaluation

We now want to infer the final imputed values to compare them to the other models. To compute these values we use the average of the values extracted for each of the missing points.

```{r, fig.width = 11, fig.height = 10, echo = FALSE}

X.full.normreg = X.full.normregOR
X.full.normreg[is.na(X.full.normregOR)] =  apply(X.MISS.rnormreg, 3, mean)
colnames(X.full.normreg) = colnames(masked_data)

X.full.normreg = X.full.normreg*scaled_scale_normregOR + scaled_center_normregOR

X.full.normreg[,2] = 2/(X.full.normreg[,2] + 1) + min(masked_data[,2], na.rm = T) - 2/10
X.full.normreg[,3] = 2/(X.full.normreg[,3] + 1) + min(masked_data[,3], na.rm = T) - 2/10
X.full.normreg[,4] = -2/(X.full.normreg[,4] + 1) + max(masked_data[,4], na.rm = T) + 2/10
X.full.normreg[,5] = 2/(X.full.normreg[,5] + 1) + min(masked_data[,5], na.rm = T) - 2/10



pairs(X.full.normreg, panel = panel.cor, diag.panel = panel.hist, main = "Model 5")

```

Oddly, many outliers were imputed, so the results are just inadequate. Let's try to replace the outliers hoping for the best.

```{r, fig.width = 11, fig.height = 10, echo = FALSE}
out <- boxplot.stats(X.full.normreg[,2])$out
out_ind <- which(X.full.normreg[,2] %in% c(out))
X.full.normreg[out_ind,2] = mean(X.full.normreg[,2])

out <- boxplot.stats(X.full.normreg[,3])$out
out_ind <- which(X.full.normreg[,3] %in% c(out))
X.full.normreg[out_ind,3] = mean(X.full.normreg[,3])

out <- boxplot.stats(X.full.normreg[,4])$out
out_ind <- which(X.full.normreg[,4] %in% c(out))
X.full.normreg[out_ind,4] = mean(X.full.normreg[,4])

out <- boxplot.stats(X.full.normreg[,5])$out
out_ind <- which(X.full.normreg[,5] %in% c(out))
X.full.normreg[out_ind,5] = mean(X.full.normreg[,5])

pairs(X.full.normreg, panel = panel.cor, diag.panel = panel.hist, main = "Model 5")
```

It's still pretty bad.

Let's see how it performs with respect to the "true" masked values.

```{r, fig.width = 6, fig.height = 4, echo = FALSE}

compute_MSE = function(x) {
  cat('MSE of the imputed data wrt the correct ones:\n')
  res = sqrt(colSums((x[,-1] - data[,-1])**2)/colSums(is.na(masked_data[,-1])))
  
  res2 = c(sqrt(sum((x - data)**2)/sum(is.na(masked_data))), res)
  names(res2) = c('Overall', names(res))
  
  res2
}

absolute_deviation_distro = function(X) {
  
  par(mfrow = c(2,2))
  
  for(i in 2:5) {
    hist(abs(X[,i] - data[,i])[is.na(masked_data[,i])], xlab = 'Absolute Deviation', main = colnames(X)[i], probability = FALSE, col = somecolors[[i]], border = somecolornames[i])
  }
  
  par(mfrow = c(1,1))
}


absolute_deviation_distro(X.full.normreg)


compute_MSE(X.full.normreg)

```

This imputation method is worse than the frequentist approach in terms of MSE. It's unexpected and it's probably due to an error or to some numerical instability due to the $\frac1x$ transform.